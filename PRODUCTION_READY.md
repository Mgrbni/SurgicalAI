# SurgicalAI Dual LLM Provider System - Production Ready

## ðŸš€ **System Overview**

**COMPLETE**: Hardened dual LLM provider system with OpenAI + Anthropic support, automatic fallback, cost tracking, streaming, schema validation, and enterprise-grade error handling.

## ðŸ—ï¸ **Architecture Implemented**

### **1. LLM Provider Abstraction (`server/llm/`)**
- **Protocol-based design** with `LLMClient` interface
- **OpenAI Client**: Full GPT-4o/GPT-4o-mini support with vision
- **Anthropic Client**: Claude 3.5 Sonnet support with vision
- **Router**: Automatic fallback with exponential backoff
- **Cost Tracking**: Real-time usage monitoring with JSONL logging

### **2. Enhanced Configuration (`server/settings.py`)**
- **Dual provider support**: Environment-based selection
- **Cost table**: Accurate pricing for OpenAI and Anthropic models
- **Security**: API key redaction and secure logging
- **Environment expansion**: Variable substitution with defaults

### **3. Schema Validation (`server/schemas.py`)**
- **Pydantic v2 models** with strict validation
- **Structured outputs**: Diagnosis probabilities, warnings, citations
- **JSON-only enforcement**: Auto-repair for malformed responses
- **Type safety**: Annotated fields with constraints

### **4. Usage Monitoring (`server/usage.py`)**
- **JSONL logging**: Token usage, costs, latency tracking
- **Console output**: Real-time provider/model/cost display
- **Request IDs**: End-to-end traceability
- **Fallback tracking**: Clear visibility when fallback used

### **5. Enhanced API (`server/server.py`)**
- **Streaming support**: Server-Sent Events for real-time analysis
- **Request IDs**: UUID tracking across all operations
- **Rate limiting**: Simple token bucket protection
- **Security headers**: CORS restrictions, redacted error messages
- **Clinical integration**: ABCDE features + H-zone mapping

## ðŸ“‹ **Quick Start Runbook**

### **Step 1: Environment Setup**
```bash
# Copy and configure environment
cp .env.example .env

# Edit .env with your API keys:
PROVIDER=openai                    # or anthropic
OPENAI_API_KEY=sk-proj-YOUR-KEY
ANTHROPIC_API_KEY=sk-ant-YOUR-KEY
OPENAI_MODEL=gpt-4o-mini
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
MAX_OUTPUT_TOKENS=1200
TIMEOUT_SECONDS=45
```

### **Step 2: Install Dependencies**
```bash
pip install -r requirements.txt
# Includes: anthropic>=0.34.0, pydantic>=2.7, tenacity>=8.2, orjson>=3.10
```

### **Step 3: Start Server**
```bash
# Option 1: Direct uvicorn
python -m uvicorn server.server:app --host 0.0.0.0 --port 7860 --reload

# Option 2: Make command (if available)
make api
```

### **Step 4: Verify System**
```bash
# Health check with provider info
curl -s http://localhost:7860/api/healthz | jq
# Expected: {"provider": "openai", "model": "gpt-4o-mini", "ok": true, ...}

# Provider configuration
curl -s http://localhost:7860/api/providers | jq
```

## ðŸ§ª **Testing & Validation**

### **Test 1: Basic Analysis**
```bash
curl -X POST http://localhost:7860/api/analyze \
  -F site="cheek" \
  -F suspected_type="seborrheic_keratosis" \
  -F image=@data/samples/lesion.jpg \
  | jq
```

**Expected Output:**
```json
{
  "diagnosis_probs": [
    {"condition": "seborrheic_keratosis", "probability": 0.75},
    {"condition": "basal_cell_carcinoma", "probability": 0.15},
    {"condition": "melanoma", "probability": 0.10}
  ],
  "primary_dx": "seborrheic_keratosis",
  "warnings": ["Monitor for asymmetry changes"],
  "citations": ["Dermatology Atlas, 2024"],
  "_metadata": {
    "request_id": "uuid-here",
    "provider": "openai",
    "model": "gpt-4o-mini",
    "processing_time_ms": 2150,
    "fallback_used": false
  }
}
```

### **Test 2: Streaming Analysis**
```bash
# Browser: http://localhost:7860/?stream=1
# Or curl:
curl -X POST "http://localhost:7860/api/analyze?stream=1" \
  -F site="nose" \
  -F image=@data/samples/lesion.jpg
```

### **Test 3: Fallback Drill**
```bash
# 1. Temporarily set bad OpenAI key in .env:
OPENAI_API_KEY=sk-invalid-key-test

# 2. Ensure Anthropic key is valid
# 3. Make analysis request
# 4. Check logs for "fallback used" message
```

### **Test 4: Cost & Usage Tracking**
```bash
# Check usage logs
tail -n 5 runs/logs/usage.jsonl | jq

# API usage stats
curl -s http://localhost:7860/api/usage | jq
```

## ðŸ”’ **Security Features**

### **API Key Protection**
- âœ… Keys redacted in all logs and error messages
- âœ… Environment-only configuration (no hardcoded secrets)
- âœ… Error messages sanitized for production

### **Rate Limiting & DoS Protection**
- âœ… Simple rate limiting (1 request/second per session)
- âœ… Timeout enforcement (45 seconds default)
- âœ… Request size validation

### **CORS & Access Control**
- âœ… Restricted origins for demo (localhost only)
- âœ… Method whitelisting (GET/POST only)
- âœ… Request ID tracking for audit

## ðŸ’° **Cost Monitoring**

### **Built-in Cost Table**
```python
COST_TABLE = {
    "openai": {
        "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},  # USD per 1K tokens
        "gpt-4o": {"input": 0.0025, "output": 0.010}
    },
    "anthropic": {
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015}
    }
}
```

### **Usage Logging Format**
```json
{
  "request_id": "uuid-4567",
  "provider": "openai",
  "model": "gpt-4o-mini", 
  "input_tokens": 512,
  "output_tokens": 420,
  "cost_usd": 0.0126,
  "latency_ms": 2100,
  "timestamp": 1691234567.89,
  "fallback_used": false,
  "success": true
}
```

### **Console Output**
```
PROVIDER=openai MODEL=gpt-4o-mini in=512 out=420 cost=$0.0126 latency=2.1s (fallback=false)
```

## ðŸŽ¯ **Production Readiness Checklist**

### **âœ… Core Features**
- [x] Dual provider support (OpenAI + Anthropic)
- [x] Automatic fallback with retry logic
- [x] Streaming analysis with SSE
- [x] Schema validation with Pydantic v2
- [x] Cost tracking and usage monitoring
- [x] Request ID tracing
- [x] Clinical feature integration (ABCDE + H-zone)

### **âœ… Security & Reliability**
- [x] API key redaction and secure logging
- [x] Rate limiting and timeout enforcement
- [x] CORS restrictions for demo environment
- [x] Error handling with graceful degradation
- [x] Input validation and sanitization

### **âœ… Observability**
- [x] Structured logging with JSONL format
- [x] Real-time console output with cost info
- [x] Usage statistics API endpoint
- [x] Health check with provider status
- [x] Request tracing with UUIDs

### **âœ… Developer Experience**
- [x] Environment-based configuration
- [x] Hot reload support for development
- [x] Comprehensive error messages
- [x] API documentation with examples
- [x] Test suite for validation

## ðŸš¦ **System Status**

**ðŸŸ¢ READY FOR DEMO**: All production features implemented and tested.

**Next Phase Options:**
1. **Dashboard Integration**: React UI for unified analysis workflow
2. **Advanced Monitoring**: Prometheus metrics and alerting
3. **Clinical Integration**: FHIR compatibility and EHR workflows
4. **Scale Testing**: Load testing with concurrent requests

## ðŸ“ž **Support & Troubleshooting**

### **Common Issues**
1. **Import errors**: Ensure all dependencies installed with `pip install -r requirements.txt`
2. **API key issues**: Check .env file format and key validity
3. **Port conflicts**: Change port with `--port 8000` if 7860 busy
4. **Timeout errors**: Increase `TIMEOUT_SECONDS` in .env

### **Debug Commands**
```bash
# Check server logs
tail -f runs/logs/usage.jsonl

# Test basic connectivity  
curl http://localhost:7860/api/healthz

# Validate environment
python test_basic_system.py
```

**ðŸŽ‰ SYSTEM READY FOR PRODUCTION DEMO** ðŸŽ‰

## Dashboard UX

The dashboard uses Tailwind (CDN) and Alpine.js with a small API client. Theming is handled via CSS variables on the html element and a theme-* class for compatibility with existing styles.

- Theme tokens: --bg, --card, --text, --muted, --border, --accent, --accent-contrast, --ring
- Themes: light, dark, clinic (accent #2fa58c on #f7faf9 background)
- Persistence: localStorage key surgicalai_theme

Port auto-detection: the client computes API_BASE as protocol + hostname + :7860 by default and allows overriding via window.SURGICALAI_API_BASE. All client fetches use this base. CORS allows localhost/127.0.0.1 on ports 8000/5173/8080 for easy local serving from static file servers or dev servers.
